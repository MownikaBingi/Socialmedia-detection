# -*- coding: utf-8 -*-
"""Ml_model(instagram).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xcPaqcrKuwPkBqUAUWShmSkbSyNOBvvV
"""

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

import keras
from sklearn.preprocessing import StandardScaler , LabelEncoder
from keras.models import Model, Sequential
from keras.layers import Dense, Activation, Flatten, BatchNormalization, Dropout

train_path = '/content/train.csv'
test_path = '/content/test.csv'

train = pd.read_csv(train_path)
test = pd.read_csv(test_path)

train.head(10)

train.tail(10)

train.iloc[:]

train.info()

x_train = train.iloc[:, :-1].values
y_train = train.iloc[:, -1].values

x_test = test.iloc[:, :-1].values
y_test = test.iloc[:, -1].values

np.save('x_train.npy', x_train)
np.save('y_train.npy', y_train)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train_sc = sc.fit_transform(x_train)
x_test_sc = sc.transform(x_test)

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
x_train_sc_pca = pca.fit_transform(x_train_sc)
x_test_sc_pca = pca.transform(x_test_sc)

losreg = LogisticRegression(random_state = 7)
losreg.fit(x_train_sc_pca, y_train)
y_pred = losreg.predict(x_test_sc_pca)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

knn = KNeighborsClassifier(n_neighbors =9, metric = 'minkowski', p = 2)
knn.fit(x_train_sc, y_train)
y_pred = knn.predict(x_test_sc)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

svc = SVC(kernel = 'rbf', random_state = 5)
svc.fit(x_train_sc, y_train)
y_pred = svc.predict(x_test_sc)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

dtc = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
dtc.fit(x_train_sc, y_train)
y_pred = dtc.predict(x_test_sc)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

rfc = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
rfc.fit(x_train_sc, y_train)
y_pred = rfc.predict(x_test_sc)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""profile_pic = input('0 is no profile pic, 1 is profile pic is available : ') num_by_num = input('ratio of number of numerical chars in username to its length : ') full_name = input('full name in word tokens : ') num_by_char = input('ratio of number of numerical characters in full name to its length : ') name_username = input('are username and full name literally the same 0 : if not simillar and 1 : if simillar : ') bio_len = input('bio length in characters : ') url = input('has external URL or not 0 : dont have and 1 : have url : ') private = input('0 : if not private , 1 : if private : ') post = input('number of post : ') followers = input('number of followers : ') follows = input('number of follows : ')

"""

pip install instaloader

import instaloader
import numpy as np

# Assuming 'sc' is the StandardScaler used during training
# Assuming 'rfc' is the trained Random Forest Classifier

# Step 1: Fetch details from Instagram profile using Instaloader

# Create an Instaloader instance
ig = instaloader.Instaloader()

# Get the username from user input
username = input("Enter Username: ")

# Fetch profile information
profile = instaloader.Profile.from_username(ig.context, username)

# Display username and related details
print("Username:", profile.username)

# Count the number of digits and words in the username
num_digits_in_username = sum(char.isdigit() for char in profile.username)
print("Number of digits in username:", num_digits_in_username)


num_words_in_username = len(profile.username.split())
print("Number of words in username:", num_words_in_username)

# Count the number of digits and words in the full name
num_digits_in_full_name = sum(char.isdigit() for char in profile.full_name)
print("Number of digits in full name:", num_digits_in_full_name)

num_words_in_full_name = len(profile.full_name.split())
print("Number of words in full name:", num_words_in_full_name)

# Display full name, number of posts, and bio
print("Full Name:", profile.full_name)
print("Number of Posts Uploads:", profile.mediacount)
print("Bio:", profile.biography)

# Check if the account has an external URL
if profile.external_url:
    print("External URL:", profile.external_url)
else:
    print("No External URL set.")

# Check if the account is private
if profile.is_private:
    print("Account is private.")
else:
    print("Account is not private.")

# Step 2: Use the fetched details as input to the machine learning model

# Get user input for a new profile
profile_data = {
    'profile_pic': 1 if profile.profile_pic_url else 0,
    'num_by_num': num_digits_in_username / num_words_in_username,
    'full_name': num_words_in_full_name,
    'num_by_char': num_digits_in_full_name / len(profile.full_name),
    'name_username': 1 if profile.full_name.lower() == profile.username.lower() else 0,
    'bio_len': len(profile.biography),
    'url': 1 if profile.external_url else 0,
    'private': 1 if profile.is_private else 0,
    'post': profile.mediacount,
    'followers': profile.followers,
    'follows': profile.followees,
}

# Convert the user input to a NumPy array
user_input = np.array([[profile_data['profile_pic'], profile_data['num_by_num'], profile_data['full_name'],
                        profile_data['num_by_char'], profile_data['name_username'], profile_data['bio_len'],
                        profile_data['url'], profile_data['private'], profile_data['post'],
                        profile_data['followers'], profile_data['follows']]])

# Standardize the user input using the same scaler used during training
user_input_scaled = sc.transform(user_input)

# Make predictions for the user input
prediction = rfc.predict(user_input_scaled)


prob = rfc.predict_proba(user_input_scaled)


# Display the result
if prediction == 0:
    print('0: Genuine account')
else:
    print('1: Spam account')

prob_percentage = prob[:, prediction] * 100
percentage_value = prob_percentage.item()  # Extracting scalar value from the numpy array

print('Probability : {:.0f}%'.format(percentage_value))



"""
Don't run the code bellow..........

"""

from sklearn.model_selection import GridSearchCV
param_grid = {
    'C': [0.1, 1, 10, 100],           # Regularization parameter
    'gamma': [1, 0.1, 0.01, 0.001],   # Kernel coefficient
    'kernel': ['rbf', 'linear', 'poly']  # Kernel type
}

# Create an SVC instance
svc = SVC(random_state=5)

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5)

# Fit the grid search to the data
grid_search.fit(x_train_sc, y_train)

# Get the best parameters from the grid search
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best parameters to create the final SVC model
final_svc = SVC(**best_params, random_state=5)
final_svc.fit(x_train_sc, y_train)

# Make predictions using the final model
y_pred = final_svc.predict(x_test_sc)

# Evaluate the model
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.model_selection import GridSearchCV

# Hyperparameter tuning for SVC
param_grid = {
    'C': [0.1, 1, 10, 100],           # Regularization parameter
    'gamma': ['scale', 'auto', 1, 0.1, 0.01, 0.001],   # Kernel coefficient
    'kernel': ['rbf', 'linear', 'poly']  # Kernel type
}

# Create an SVC instance
svc = SVC(random_state=5)

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5)

# Fit the grid search to the data
grid_search.fit(x_train_sc, y_train)

# Get the best parameters from the grid search
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best parameters to create the final SVC model
final_svc = SVC(**best_params, random_state=5)
final_svc.fit(x_train_sc, y_train)

# Make predictions using the final model
y_pred = final_svc.predict(x_test_sc)

# Evaluate the model
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

import tensorflow as tf

# Assuming raw_output is the raw output from your model
raw_output = rfc.predict(0)

# Applying sigmoid function for binary classification
probability = tf.nn.sigmoid(raw_output)

# If you're dealing with a multi-class classification problem, use softmax instead
# probability = tf.nn.softmax(raw_output)

from tensorflow.keras.models import save_model

# Save the Keras model
save_model(rfc, '/content/mrfc.h5')